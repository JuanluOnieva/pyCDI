{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indicators for Chronic Disease Surveillance\n",
    "\n",
    "## Authors\n",
    "\n",
    "Juan Luis Onieva Zafra\n",
    "\n",
    "Jesús Gómez Sola\n",
    "\n",
    "Paloma Domínguez Sánchez\n",
    "\n",
    "## Abstract\n",
    "\n",
    "In this proyect we present an analysis of data of indicators of chronic diseases that are provided in the data.gov portal at the address <https://catalog.data.gov/dataset/u-s-chronic-disease-indicators-cdi>, from where we have downloaded the CSV file. \n",
    "\n",
    "<img src=\"MMWR.png\" width=\"600\">\n",
    "\n",
    "The objective of the task is to use Spark to obtain various queries and represent them in a table and graph format. For this purpose, we are going to work with two different APIs: RDDs and datasets.\n",
    "\n",
    "Our work is divided into: \n",
    "\n",
    "- **Tests**:\n",
    "\n",
    "    *__init__.py*\n",
    "\n",
    "    *AnalysisTest.py*\n",
    "\n",
    "    *ReadCSVTest.py*\n",
    "\n",
    "- **CDI**:\n",
    "\n",
    "    *ReadCSV.py*\n",
    "    \n",
    "    *Analysis.py*\n",
    "        \n",
    "    *cdi.py*\n",
    "    \n",
    "    \n",
    "- LICENSE\n",
    "\n",
    "- README.md\n",
    "\n",
    "- requirements.txt\n",
    "\n",
    "- setup.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDI\n",
    "\n",
    "#### ReadCSV.py\n",
    "\n",
    "This class is in charge of reading the CSV file format as RDD or data_frame. For this purpose the class create a spark_session object which read a file in format 'CSV' that is passed as parameter.\n",
    "\n",
    "We can see defined two functions:\n",
    "\n",
    "\n",
    "*def read_csv_with_data_frame(file_csv: str) -> DataFrame*\n",
    "\n",
    "*def read_csv_with_rdd(file_csv: str) -> SparkContext*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains the function to read the csv with different methods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from py4j.protocol import Py4JError\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "\n",
    "def read_csv_with_data_frame(file_csv: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV with as data frame with spark\n",
    "    :param file_csv: file name of csv\n",
    "    :return: all the data of the file as data frame\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    logger = spark_session._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)\n",
    "\n",
    "    try:\n",
    "        data_frame = spark_session\\\n",
    "            .read\\\n",
    "            .format(\"csv\") \\\n",
    "            .options(header='true', inferschema='true')\\\n",
    "            .load(file_csv)\n",
    "    except Py4JError:\n",
    "        raise AnalysisException('There is no csv file in:'  + str(os.path))\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def read_csv_with_rdd(file_csv: str) -> SparkContext:\n",
    "    \"\"\"\n",
    "    Read csv file with rdd, then take only the columns 5 (TopicID) and 6 (Question) and produce a list of tuples\n",
    "    :param file_csv: file name of csv\n",
    "    :return: list of tuples (TopicID, Question)s\n",
    "    \"\"\"\n",
    "    spark_conf = SparkConf()\n",
    "    spark_context = SparkContext(conf=spark_conf)\n",
    "    logger = spark_context._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)\n",
    "    rdd = spark_context \\\n",
    "        .textFile(file_csv)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda row: row!=header) \\\n",
    "        .map(lambda line: line.split(\",\")) \\\n",
    "        .map(lambda line: (line[4], line[6])) \\\n",
    "        .distinct() \\\n",
    "        .map(lambda list: (list[0], 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .sortBy(lambda pair: pair[0]) \\\n",
    "        .collect()\n",
    "    spark_context.stop()\n",
    "    return rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis.py\n",
    "This class has defined several descriptive functions responsible for executing the corresponding queries:\n",
    "\n",
    "\" def get_data_frame_count_type_of_topic(data_frame: DataFrame) -> DataFrame \" returns the number of type of diseases. \n",
    "\n",
    "\" def get_data_frame_count_male_gender_by_topic(data_frame: DataFrame) -> DataFrame \" returns the number of men that has each disease.\n",
    "\n",
    "\" def get_data_frame_count_black_ethnicity_by_topic(data_frame: DataFrame) -> DataFrame \" returns the number of black ethnicity people that has each disease:\n",
    "\n",
    "\n",
    "The last function is responsible for graphically representing previously defended functions:\n",
    "\n",
    "def plot_type_of_topic(data_frame: DataFrame) -> None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains the necessary functions to deal with the data, obtain data frame and show some graphics\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from py4j.protocol import Py4JError\n",
    "import pandas as pb\n",
    "\n",
    "\n",
    "def get_data_frame_count_type_of_topic(data_frame: DataFrame) -> pb.DataFrame:\n",
    "    \"\"\"\n",
    "    From all the data, it takes the columns TopicID and Question and for each topic, count the number of+\n",
    "    different SubTopic/Question\n",
    "    :param data_frame: generate with pyspark, and contain all the data from the csv file\n",
    "    :return: data frame of panda package\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_frame = data_frame \\\n",
    "            .select(\"TopicID\", \"Question\") \\\n",
    "            .distinct() \\\n",
    "            .groupBy(\"TopicID\") \\\n",
    "            .count() \\\n",
    "            .sort(\"TopicID\")\n",
    "    except Py4JError:\n",
    "        raise AnalysisException('One columns is incorrect')\n",
    "    print(\"The following table represent the number of the type of each topic\")\n",
    "    data_frame.show()\n",
    "    data_frame_pandas = data_frame.toPandas()\n",
    "    return data_frame_pandas\n",
    "\n",
    "\n",
    "def get_rdd_count_type_of_topy(rdd: list) -> pb.DataFrame:\n",
    "    \"\"\"\n",
    "    Take an specific list from rdd spark, which is formed as list of tuples (Topic, Question)\n",
    "    :param rdd: list of tuples(Topic, Question)\n",
    "    :return: data frame of package Pandas\n",
    "    \"\"\"\n",
    "    data_frame_pandas = pb.DataFrame(rdd, columns=['Topic', 'Question'])\n",
    "    print(data_frame_pandas)\n",
    "    return data_frame_pandas\n",
    "\n",
    "\n",
    "def get_data_frame_count_male_gender_by_topic(data_frame: DataFrame) -> pb.DataFrame:\n",
    "    \"\"\"\n",
    "    From all the data, it takes the columns TopicID, and count the topic based on the gender\n",
    "    :param data_frame: generate with pyspark, and contain all the data from the csv file\n",
    "    :return: data frame of panda package\n",
    "    \"\"\"\n",
    "    data_frame_topic = data_frame \\\n",
    "        .filter(data_frame[\"Stratification1\"].contains(\"Male\")) \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"TopicID\") \\\n",
    "        .count() \\\n",
    "        .sort(\"TopicID\")\n",
    "\n",
    "    print(\"The following table represent the number of men group by the topic: \")\n",
    "    data_frame_topic.show()\n",
    "    data_frame_pandas = data_frame.toPandas()\n",
    "    return data_frame_pandas\n",
    "\n",
    "\n",
    "def get_data_frame_count_black_ethnicity_by_topic(data_frame: DataFrame) -> pb.DataFrame:\n",
    "    \"\"\"\n",
    "    From all the data, it takes the columns TopicID, and count the topic based on the ethnicity\n",
    "    :param data_frame: generate with pyspark, and contain all the data from the csv file\n",
    "    :return: data frame of panda package\n",
    "    \"\"\"\n",
    "    data_frame_topic = data_frame \\\n",
    "        .filter(data_frame[\"Stratification1\"].contains(\"Black, non-Hispanic\")) \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"TopicID\") \\\n",
    "        .count() \\\n",
    "        .sort(\"TopicID\")\n",
    "\n",
    "    print(\"The following table represent the number of black ethnicity people group by the topic: \")\n",
    "    data_frame_topic.show()\n",
    "    data_frame_pandas = data_frame.toPandas()\n",
    "    return data_frame_pandas\n",
    "\n",
    "\n",
    "def plot_type_of_topic(data_frame: pb.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot a data frame with bar type\n",
    "    :param data_frame:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    plt.interactive(False)\n",
    "    plt.figure()\n",
    "    data_frame.plot(kind='bar', x= data_frame['TopicID'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cdi.py\n",
    "\n",
    "This is the MAIN class of the proyect which is in charge of join the rest of classes. \n",
    "\n",
    "We define a data_frame object for each query and for each of them we call the function \"plot_type_of_topic\" to represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv = 'cdi/data/Chronic_Disease_Indicators_CDI.csv'\n",
    "data_frame = read_csv_with_data_frame(file_csv)\n",
    "data_frame_count_type = get_data_frame_count_type_of_topic(data_frame)\n",
    "plot_type_of_topic(data_frame_count_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph correspond to all types of diseases of the study. As we can see the most abundant disease is NPAW and the least one is DIS and IMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following table represent the number of men group by the topic: \n",
      "+-------+-----+\n",
      "|TopicID|count|\n",
      "+-------+-----+\n",
      "|    ALC| 3405|\n",
      "|    ART| 5280|\n",
      "|    AST| 4857|\n",
      "|    CAN|  440|\n",
      "|    CKD| 1425|\n",
      "|   COPD| 9792|\n",
      "|    CVD| 9333|\n",
      "|    DIA| 9732|\n",
      "|    DIS|  371|\n",
      "|    IMM|  660|\n",
      "|    MTH|  660|\n",
      "|   NPAW| 3630|\n",
      "|    OLD| 1920|\n",
      "|    ORH| 1424|\n",
      "|    OVC| 4601|\n",
      "|    TOB| 3300|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame_count_men = get_data_frame_count_male_gender_by_topic(data_frame)\n",
    "plot_type_of_topic(data_frame_count_men)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph correspond to number of men that have each disease. As we can see the most abundant disease in men are COPD and DIA. The least abundant are DIS and CAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_count_black_ethnicity = get_data_frame_count_black_ethnicity_by_topic(data_frame)\n",
    "plot_type_of_topic(data_frame_count_black_ethnicity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "#### AnalysisTest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.data_frame = read_csv_with_data_frame('data/pruebas.csv')\n",
    "        self.data_frame_wrong = read_csv_with_data_frame('data/pruebas-wrong-column.csv')\n",
    "\n",
    "    def test_when_count_subtopic_data_frame_should_have_at_least_columns_with_topic_and_subtopic(self):\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            get_data_frame_count_type_of_topic(self.data_frame_wrong)\n",
    "\n",
    "    def test_the_number_of_topic_must_be_correct(self):\n",
    "        data_frame_topic = get_data_frame_count_type_of_topic(self.data_frame)\n",
    "        total = data_frame_topic.count()\n",
    "        expected_value = 3\n",
    "        self.assertEqual(expected_value, total)\n",
    "\n",
    "    def test_the_total_number_must_correspond_with_size_of_csv(self):\n",
    "        data_frame_topic = get_data_frame_count_type_of_topic(self.data_frame)\n",
    "        data_frame_pandas = data_frame_topic.toPandas()\n",
    "        total = sum(data_frame_pandas['count'])\n",
    "        expected_value = 6\n",
    "        self.assertEqual(expected_value, total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReadCSVTest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "\n",
    "    def test_read_csv_from_data_frame_read_correctly(self):\n",
    "        data_frame = read_csv_with_data_frame('data/pruebas.csv')\n",
    "        data_frame_total = data_frame \\\n",
    "            .count()\n",
    "        expected_value = 12\n",
    "        self.assertEqual(expected_value, data_frame_total)\n",
    "\n",
    "    def test_raise_exception_when_the_file_is_not_csv(self):\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            read_csv_with_data_frame('data/pruebas.tsv')\n",
    "\n",
    "    def test_raise_exception_when_the_file_not_exist(self):\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            read_csv_with_data_frame('data/no-file.tsv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
