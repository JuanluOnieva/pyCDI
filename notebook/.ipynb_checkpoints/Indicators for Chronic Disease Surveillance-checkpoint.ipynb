{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indicators for Chronic Disease Surveillance\n",
    "\n",
    "## Authors\n",
    "\n",
    "Juan Luis Onieva Zafra\n",
    "\n",
    "Jesús Gómez Sola\n",
    "\n",
    "Paloma Domínguez Sánchez\n",
    "\n",
    "## Abstract\n",
    "\n",
    "In this proyect we present an analysis of data of indicators of chronic diseases that are provided in the data.gov portal at the address <https://catalog.data.gov/dataset/u-s-chronic-disease-indicators-cdi>, from where we have downloaded the CSV file. \n",
    "\n",
    "<img src=\"MMWR.png\" width=\"600\">\n",
    "\n",
    "The objective of the task is to use Spark to obtain various queries and represent them in a table and graph format. For this purpose, we are going to work with two different APIs: RDDs and datasets.\n",
    "\n",
    "Our work is divided into: \n",
    "\n",
    "- **Tests**:\n",
    "\n",
    "    *__init__.py*\n",
    "\n",
    "    *AnalysisTest.py*\n",
    "\n",
    "    *ReadCSVTest.py*\n",
    "\n",
    "- **CDI**:\n",
    "\n",
    "    *ReadCSV.py*\n",
    "    \n",
    "    *Analysis.py*\n",
    "        \n",
    "    *cdi.py*\n",
    "    \n",
    "    \n",
    "- LICENSE\n",
    "\n",
    "- README.md\n",
    "\n",
    "- requirements.txt\n",
    "\n",
    "- setup.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDI\n",
    "\n",
    "#### ReadCSV.py\n",
    "\n",
    "This class is in charge of reading the CSV file format as RDD or data_frame. For this purpose the class create a spark_session object which read a file in format 'CSV' that is passed as parameter.\n",
    "\n",
    "We can see defined two functions:\n",
    "\n",
    "\n",
    "*def read_csv_with_data_frame(file_csv: str) -> DataFrame*\n",
    "\n",
    "*def read_csv_with_rdd(file_csv: str) -> SparkContext*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from py4j.protocol import Py4JError\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def read_csv_with_data_frame(file_csv: str) -> DataFrame:\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    logger = spark_session._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)\n",
    "\n",
    "    try:\n",
    "        data_frame = spark_session\\\n",
    "            .read\\\n",
    "            .format(\"csv\") \\\n",
    "            .options(header='true', inferschema='true')\\\n",
    "            .load(file_csv)\n",
    "    except Py4JError:\n",
    "        raise AnalysisException('There is no csv file in:'  + str(os.path))\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def read_csv_with_rdd(file_csv: str) -> SparkContext:\n",
    "    spark_conf = SparkConf()\n",
    "    spark_context = SparkContext(conf=spark_conf)\n",
    "\n",
    "    logger = spark_context._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)\n",
    "\n",
    "    rdd = spark_context \\\n",
    "        .textFile(file_csv)\n",
    "    header = rdd.first()\n",
    "    return rdd\n",
    "\n",
    "    rdd = rdd.filter(lambda row: row!=header) \\\n",
    "        .map(lambda line: line.split(\",\")) \\\n",
    "        .map(lambda line: (line[5], line[6])) \\\n",
    "        .distinct() \\\n",
    "        .map(lambda list: (list[0], 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y) \\\n",
    "        .sortBy(lambda pair: pair[0]) \\\n",
    "        .collect()\n",
    "    # .map(lambda line: (line[0], (line[1], 1))) \\\n",
    "    # .reduceByKey(lambda x, y: x + y) \\\n",
    "    spark_context.stop()\n",
    "    print(rdd)\n",
    "    return rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis.py\n",
    "This class has defined several descriptive functions responsible for executing the corresponding queries:\n",
    "\n",
    "\" def get_data_frame_count_type_of_topic(data_frame: DataFrame) -> DataFrame \" returns the number of type of diseases. \n",
    "\n",
    "\" def get_data_frame_count_male_gender_by_topic(data_frame: DataFrame) -> DataFrame \" returns the number of men that has each disease.\n",
    "\n",
    "\" def get_data_frame_count_black_ethnicity_by_topic(data_frame: DataFrame) -> DataFrame \" returns the number of black ethnicity people that has each disease:\n",
    "\n",
    "\n",
    "The last function is responsible for graphically representing previously defended functions:\n",
    "\n",
    "def plot_type_of_topic(data_frame: DataFrame) -> None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas\n",
    "\n",
    "\n",
    "def get_data_frame_count_type_of_topic(data_frame: DataFrame) -> DataFrame:\n",
    "    data_frame_topic = data_frame \\\n",
    "        .select(\"TopicID\", \"Question\") \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"TopicID\") \\\n",
    "        .count() \\\n",
    "        .sort(\"TopicID\")\n",
    "\n",
    "    print(\"The following table represent the number of the type of each topic\")\n",
    "    data_frame_topic.show()\n",
    "    return data_frame_topic\n",
    "\n",
    "def get_data_frame_count_male_gender_by_topic(data_frame: DataFrame) -> DataFrame:\n",
    "    data_frame_topic = data_frame \\\n",
    "        .filter(data_frame[\"Stratification1\"].contains(\"Male\")) \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"TopicID\") \\\n",
    "        .count() \\\n",
    "        .sort(\"TopicID\")\n",
    "\n",
    "    print(\"The following table represent the number of men group by the topic: \")\n",
    "    data_frame_topic.show()\n",
    "    return data_frame_topic\n",
    "\n",
    "def get_data_frame_count_black_ethnicity_by_topic(data_frame: DataFrame) -> DataFrame:\n",
    "    data_frame_topic = data_frame \\\n",
    "        .filter(data_frame[\"Stratification1\"].contains(\"Black, non-Hispanic\")) \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"TopicID\") \\\n",
    "        .count() \\\n",
    "        .sort(\"TopicID\")\n",
    "\n",
    "    print(\"The following table represent the number of black ethnicity people group by the topic: \")\n",
    "    data_frame_topic.show()\n",
    "    return data_frame_topic\n",
    "\n",
    "\n",
    "def plot_type_of_topic(data_frame: DataFrame) -> None:\n",
    "    data_frame_pandas = data_frame.toPandas()\n",
    "    for row in data_frame_pandas:\n",
    "        print(row)\n",
    "    print(data_frame_pandas)\n",
    "    plt.interactive(False)\n",
    "    plt.figure()\n",
    "    data_frame_pandas.plot(kind='bar', x=data_frame_pandas['TopicID'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cdi.py\n",
    "\n",
    "This is the MAIN class of the proyect which is in charge of join the rest of classes. \n",
    "\n",
    "We define a data_frame object for each query and for each of them we call the function \"plot_type_of_topic\" to represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/Paloma/Documents/bitbucket/pycdipab2018/Chronic_Disease_Indicators_CDI.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o125.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/Paloma/Documents/bitbucket/pycdipab2018/Chronic_Disease_Indicators_CDI.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2f83ad685645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Chronic_Disease_Indicators_CDI.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#file_csv = 'data/cdi.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv_with_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_frame_count_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_frame_count_type_of_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_type_of_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame_count_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-6554a9e2bce1>\u001b[0m in \u001b[0;36mread_csv_with_data_frame\u001b[0;34m(file_csv)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_session\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There is no csv file in:'\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/Paloma/Documents/bitbucket/pycdipab2018/Chronic_Disease_Indicators_CDI.csv;'"
     ]
    }
   ],
   "source": [
    "file_csv = 'Chronic_Disease_Indicators_CDI.csv'\n",
    "#file_csv = 'data/cdi.csv'\n",
    "data_frame = read_csv_with_data_frame(file_csv)\n",
    "data_frame_count_type = get_data_frame_count_type_of_topic(data_frame)\n",
    "plot_type_of_topic(data_frame_count_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "#### AnalysisTest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ReadCSV'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-08054cc84e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mReadCSV\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv_with_data_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mAnalysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data_frame_count_type_of_topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ReadCSV'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from ReadCSV import read_csv_with_data_frame\n",
    "from Analysis import get_data_frame_count_type_of_topic\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.data_frame = read_csv_with_data_frame('data/pruebas.csv')\n",
    "        self.data_frame_wrong = read_csv_with_data_frame('data/pruebas-wrong-column.csv')\n",
    "\n",
    "    def test_when_count_subtopic_data_frame_should_have_at_least_columns_with_topic_and_subtopic(self):\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            get_data_frame_count_type_of_topic(self.data_frame_wrong)\n",
    "\n",
    "    def test_the_number_of_topic_must_be_correct(self):\n",
    "        data_frame_topic = get_data_frame_count_type_of_topic(self.data_frame)\n",
    "        total = data_frame_topic.count()\n",
    "        expected_value = 3\n",
    "        self.assertEqual(expected_value, total)\n",
    "\n",
    "    def test_the_total_number_must_correspond_with_size_of_csv(self):\n",
    "        data_frame_topic = get_data_frame_count_type_of_topic(self.data_frame)\n",
    "        data_frame_pandas = data_frame_topic.toPandas()\n",
    "        total = sum(data_frame_pandas['count'])\n",
    "        expected_value = 6\n",
    "        self.assertEqual(expected_value, total)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReadCSVTest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ReadCSV'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-395840bc566a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mReadCSV\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv_with_data_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ReadCSV'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from ReadCSV import read_csv_with_data_frame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "\n",
    "    def test_read_csv_from_data_frame_read_correctly(self):\n",
    "        data_frame = read_csv_with_data_frame('data/pruebas.csv')\n",
    "        data_frame_total = data_frame \\\n",
    "            .count()\n",
    "        expected_value = 12\n",
    "        self.assertEqual(expected_value, data_frame_total)\n",
    "\n",
    "    def test_raise_exception_when_the_file_is_not_csv(self):\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            read_csv_with_data_frame('data/pruebas.tsv')\n",
    "\n",
    "    def test_raise_exception_when_the_file_not_exist(self):\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            read_csv_with_data_frame('data/no-file.tsv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
